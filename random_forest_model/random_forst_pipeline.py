# -*- coding: utf-8 -*-
"""Untitled77.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t5tZAHAyghpg8bPZc1HT2pktvo8BlYNU
"""

import joblib
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt
from preprocessing.text_processing_and_feature_engineering import *

class RandomForestEvaluationPipeline:
    def __init__(self, model_dir="random_forest_model", selected_features=None):
        default_selected_features = [
            'char_count', 'word_count', 'unique_word_count', 'avg_word_length', 'sentence_count',
            'symbol_to_word_ratio', 'capital_letter_ratio', 'special_char_ratio', 'comma_ratio',
            'period_ratio', 'question_mark_ratio', 'exclamation_mark_ratio', 'ellipses_ratio',
            'url_count_ratio', 'hashtag_ratio', 'at_ratio', 'quote_ratio', 'title_char_count',
            'title_word_count', 'title_unique_word_count', 'avg_word_length_title',
            'stop_word_count_title', 'stop_word_ratio_title'
        ]
        self.selected_features = selected_features or default_selected_features

        self.model_path = f"{model_dir}/random_forest_model.pkl"
        self.model = joblib.load(self.model_path)

    def preprocess(self, data):
        processed_data = preprocess_and_engineer_features(data)
        return processed_data[self.selected_features]

    def predict(self, data):
        processed_data = self.preprocess(data)
        predictions = self.model.predict(processed_data)
        probabilities = self.model.predict_proba(processed_data)[:, 1]

        return {
            "predictions": predictions,
            "probabilities": probabilities
        }

    def evaluate(self, data, true_labels):
        results = self.predict(data)
        predictions = results["predictions"]
        probabilities = results["probabilities"]

        report = classification_report(true_labels, predictions)
        conf_matrix = confusion_matrix(true_labels, predictions)
        roc_auc = roc_auc_score(true_labels, probabilities)

        plt.figure(figsize=(8, 6))
        sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["False", "True"], yticklabels=["False", "True"])
        plt.title("Confusion Matrix", fontsize=16)
        plt.xlabel("Predicted Label", fontsize=14)
        plt.ylabel("True Label", fontsize=14)
        plt.show()

        fpr, tpr, _ = roc_curve(true_labels, probabilities)
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f"ROC-AUC score: {roc_auc:.2f}", color="darkorange", lw=2)
        plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
        plt.xlabel("False Positive Rate", fontsize=12)
        plt.ylabel("True Positive Rate", fontsize=12)
        plt.title("ROC Curve", fontsize=16)
        plt.legend(loc="lower right")
        plt.grid(alpha=0.3)
        plt.show()

        return {
            "classification_report": report,
            "confusion_matrix": conf_matrix,
            "roc_auc": roc_auc
        }